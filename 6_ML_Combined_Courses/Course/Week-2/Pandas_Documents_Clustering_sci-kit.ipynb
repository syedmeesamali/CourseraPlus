{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8V5UxN6fbMuR"
      },
      "source": [
        "### Pandas equivalent example to work with Document Clustering for People Wiki dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#people = pd.read_csv(f\"D:/people_wiki.csv\")\n",
        "people = pd.read_csv(f\"D:/SYED/people_wiki.csv\")\n",
        "people.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "obama = people[people['name'] == 'Barack Obama']\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "matrix = vectorizer.fit_transform(obama['text'])\n",
        "matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counts = pd.DataFrame(matrix.toarray(), columns = vectorizer.get_feature_names_out())\n",
        "counts.T.sort_values(by=0, ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "counts['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "people.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pep1 = people[10:100]\n",
        "pep1.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pep1.rename(columns = {'text':'text_data'}, inplace = True)\n",
        "pep1.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function to make word-count as dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "def make_counts(text_data):\n",
        "    cv_fit = cv.fit_transform(text_data)\n",
        "    word_list = cv.get_feature_names_out()\n",
        "    count_list = np.asarray(cv_fit.sum(axis=0))[0]\n",
        "    final_dict = dict(zip(word_list, count_list))\n",
        "    return final_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply function to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pep1['word_count'] = pep1['text_data'].apply(lambda x: make_counts([x]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pep1['word_count'][15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pep1.tail(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "obama['word_count'] = obama['text'].apply(lambda x: make_counts([x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF using texthero Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "v = TfidfVectorizer()\n",
        "def make_tfidf(text_data):\n",
        "    xval = v.fit_transform(text_data)\n",
        "    names_val = v.get_feature_names_out()\n",
        "    return names_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\SYED\\AppData\\Local\\Temp\\ipykernel_2096\\621838335.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  pep1['tfidf'] = pep1['word_count'].apply(lambda x: make_tfidf(x))\n"
          ]
        }
      ],
      "source": [
        "pep1['tfidf'] = pep1['word_count'].apply(lambda x: make_tfidf(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1981' '1991' '1994' '2001' '2009' '2010' '27' 'about' 'acclaimed'\n",
            " 'acrobatics' 'adaptation' 'afraid' 'after' 'aline' 'americanfrench' 'an'\n",
            " 'and' 'another' 'apprenticed' 'artist' 'artists' 'as' 'at' 'born'\n",
            " 'brooklyn' 'by' 'california' 'christian' 'circus' 'clowes' 'clowning'\n",
            " 'comedydrama' 'comics' 'comix' 'commissioned' 'completing'\n",
            " 'conservatives' 'construction' 'critically' 'crumb' 'daniel' 'daughter'\n",
            " 'documentary' 'drawings' 'earned' 'education' 'eli' 'english' 'family'\n",
            " 'farming' 'father' 'film' 'for' 'foreign' 'france' 'french' 'from'\n",
            " 'fundamentalists' 'ghost' 'girlit' 'her' 'herself' 'his' 'husband' 'in'\n",
            " 'inclusion' 'inquirer' 'interview' 'into' 'is' 'kominskycrumbcrumb'\n",
            " 'languageshe' 'later' 'lived' 'lives' 'living' 'mid2000s' 'mother'\n",
            " 'nameafter' 'nearby' 'nine' 'of' 'old' 'on' 'original' 'parents'\n",
            " 'philadelphia' 'political' 'prepare' 'released' 'relocated' 'relocation'\n",
            " 'remove' 'reported' 'robert' 'same' 'sauve' 'school' 'secondary'\n",
            " 'september' 'serial' 'she' 'sold' 'some' 'son' 'sophia' 'sophie'\n",
            " 'sophies' 'southern' 'stage' 'states' 'street' 'studied' 'tattoo'\n",
            " 'teaching' 'terry' 'that' 'the' 'their' 'they' 'this' 'to' 'told' 'town'\n",
            " 'turn' 'underground' 'united' 'until' 'valley' 'village' 'violet'\n",
            " 'wanted' 'was' 'while' 'who' 'winters' 'with' 'woodland' 'worker' 'world'\n",
            " 'would' 'years' 'zwigoff']\n"
          ]
        }
      ],
      "source": [
        "print(pep1['tfidf'][10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Documents_Clustering_Colab_Turi_People-Wiki.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "19c641909dbba517c49d9f0678c83bb6138cd8161df841f54650d88c9b22b355"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
