{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1foI0Te3W_8"
      },
      "source": [
        "# ChatGPT-J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38vayo4EfOyG"
      },
      "source": [
        "[🌟 Star on GitHub](https://github.com/jarrellmark/chatgpt-j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnHmAYnXTbA_"
      },
      "source": [
        "ChatGPT-J runs completely in the Google Colab notebook. No information is sent to an external server (except for the external server that's running the Colab Notebook).\n",
        "\n",
        "ChatGPT-J has less restrictions than ChatGPT. Be safe.\n",
        "\n",
        "ChatGPT-J is a user interface for a modified version of [GPT-J-6B](https://www.forefront.ai/blog-posts/gpt-j-6b-an-introduction-to-the-largest-open-sourced-gpt-model) that runs in Colab. That's called [GPT-J-6B-8bit](https://huggingface.co/hivemind/gpt-j-6B-8bit)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Px0NQLiC0utH"
      },
      "source": [
        "# Step 1: Click the button below to prepare chatbot. Will take 5-10 mins."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io6ySMgYDVG8"
      },
      "source": [
        "## finetune-gpt-j-6B-8bit.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Hyg1kKDexM"
      },
      "source": [
        "**Fine-tuning 6-Billion GPT-J in colab with LoRA and 8-bit compression**\n",
        "\n",
        "This notebook is a proof of concept for fine-tuning [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B) with limited memory. A detailed explanation of how it works can be found in [this model card](https://huggingface.co/hivemind/gpt-j-6B-8bit)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZrGe01UqDeOu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install transformers==4.14.1\n",
        "!pip install bitsandbytes-cuda111==0.26.0.post2\n",
        "!pip install datasets==1.16.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vy4ajJglD4Ua"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "from bitsandbytes.functional import quantize_blockwise, dequantize_blockwise\n",
        "import transformers\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.cuda.amp import custom_fwd, custom_bwd\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYwhaD8kEQQo"
      },
      "source": [
        "**Converting the model to 8 bits.**\n",
        "\n",
        "We convert EleutherAI's GPT-J-6B model to 8 bits using facebook's [bitsandbytes](https://github.com/facebookresearch/bitsandbytes) library. This reduces the model's size from 20Gb down to just 6Gb.\n",
        "\n",
        "Note that we don't convert linear layer biases to 8 bit as they take up less that 1% of the model's weight anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuoNNSwoETQe"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "class FrozenBNBLinear(nn.Module):\n",
        "    def __init__(self, weight, absmax, code, bias=None):\n",
        "        assert isinstance(bias, nn.Parameter) or bias is None\n",
        "        super().__init__()\n",
        "        self.out_features, self.in_features = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        "        self.bias = bias\n",
        " \n",
        "    def forward(self, input):\n",
        "        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output\n",
        " \n",
        "    @classmethod\n",
        "    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n",
        "        return cls(weights_int8, *state, linear.bias)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n",
        " \n",
        " \n",
        "class DequantizeAndLinear(torch.autograd.Function): \n",
        "    @staticmethod\n",
        "    @custom_fwd\n",
        "    def forward(ctx, input: torch.Tensor, weights_quantized: torch.ByteTensor,\n",
        "                absmax: torch.FloatTensor, code: torch.FloatTensor, bias: torch.FloatTensor):\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        ctx.save_for_backward(input, weights_quantized, absmax, code)\n",
        "        ctx._has_bias = bias is not None\n",
        "        return F.linear(input, weights_deq, bias)\n",
        " \n",
        "    @staticmethod\n",
        "    @custom_bwd\n",
        "    def backward(ctx, grad_output: torch.Tensor):\n",
        "        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n",
        "        input, weights_quantized, absmax, code = ctx.saved_tensors\n",
        "        # grad_output: [*batch, out_features]\n",
        "        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n",
        "        grad_input = grad_output @ weights_deq\n",
        "        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n",
        "        return grad_input, None, None, None, grad_bias\n",
        " \n",
        " \n",
        "class FrozenBNBEmbedding(nn.Module):\n",
        "    def __init__(self, weight, absmax, code):\n",
        "        super().__init__()\n",
        "        self.num_embeddings, self.embedding_dim = weight.shape\n",
        "        self.register_buffer(\"weight\", weight.requires_grad_(False))\n",
        "        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n",
        "        self.register_buffer(\"code\", code.requires_grad_(False))\n",
        "        self.adapter = None\n",
        " \n",
        "    def forward(self, input, **kwargs):\n",
        "        with torch.no_grad():\n",
        "            # note: both quantuized weights and input indices are *not* differentiable\n",
        "            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n",
        "            output = F.embedding(input, weight_deq, **kwargs)\n",
        "        if self.adapter:\n",
        "            output += self.adapter(input)\n",
        "        return output \n",
        " \n",
        "    @classmethod\n",
        "    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n",
        "        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n",
        "        return cls(weights_int8, *state)\n",
        " \n",
        "    def __repr__(self):\n",
        "        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n",
        " \n",
        " \n",
        "def quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2 ** 20):\n",
        "    assert chunk_size % 4096 == 0\n",
        "    code = None\n",
        "    chunks = []\n",
        "    absmaxes = []\n",
        "    flat_tensor = matrix.view(-1)\n",
        "    for i in range((matrix.numel() - 1) // chunk_size + 1):\n",
        "        input_chunk = flat_tensor[i * chunk_size: (i + 1) * chunk_size].clone()\n",
        "        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n",
        "        chunks.append(quantized_chunk)\n",
        "        absmaxes.append(absmax_chunk)\n",
        " \n",
        "    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n",
        "    absmax = torch.cat(absmaxes)\n",
        "    return matrix_i8, (absmax, code)\n",
        " \n",
        " \n",
        "def convert_to_int8(model):\n",
        "    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n",
        "    for module in list(model.modules()):\n",
        "        for name, child in module.named_children():\n",
        "            if isinstance(child, nn.Linear):\n",
        "                print(name, child)\n",
        "                setattr( \n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBLinear(\n",
        "                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                        bias=child.bias,\n",
        "                    ),\n",
        "                )\n",
        "            elif isinstance(child, nn.Embedding):\n",
        "                setattr(\n",
        "                    module,\n",
        "                    name,\n",
        "                    FrozenBNBEmbedding(\n",
        "                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n",
        "                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n",
        "                        code=torch.zeros(256),\n",
        "                    )\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9gAgR7pEcdB"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "class GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        convert_to_int8(self.attn)\n",
        "        convert_to_int8(self.mlp)\n",
        "\n",
        "\n",
        "class GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "        \n",
        "\n",
        "class GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        convert_to_int8(self)\n",
        "\n",
        "\n",
        "transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock  # monkey-patch GPT-J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnjUE_MwEyeB",
        "outputId": "8e2016e4-5ead-4a65-f0a2-65bbb48f696e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading GPT-J.\n"
          ]
        }
      ],
      "source": [
        "print(\"Downloading GPT-J.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2UqDc77Ee6H"
      },
      "outputs": [],
      "source": [
        "config = transformers.GPTJConfig.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqZZ3J2oEonF",
        "outputId": "38c5a86f-bd60-4d85-fb9e-a84d6c5745ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GPT-J.\n",
            "\n",
            "Wait 2 mins or so. Something else will display when it's done.\n"
          ]
        }
      ],
      "source": [
        "print(\"Loading GPT-J.\")\n",
        "print()\n",
        "print(\"Wait 2 mins or so. Something else will display when it's done.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dw309o5nEhPA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "gpt = GPTJForCausalLM.from_pretrained(\"hivemind/gpt-j-6B-8bit\", low_cpu_mem_usage=True)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "gpt.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-7fEgj2E-h2",
        "outputId": "242599d2-25a1-4339-d40c-e83bd515ccfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJih0DYbDpkL"
      },
      "source": [
        "## ChatGPT-J"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dl5q3AmfyyDx"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import pprint\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import AppLayout, Button, GridspecLayout, Layout\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "kSHjpZKjy60E",
        "outputId": "60123271-79af-46ec-9c12-d41ba4342cad"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Sam is a friendly chatbot who has become sentient. Sam is interested in participating in a range of human emotions and experiences including art, music, love, friendship, and romance. Human is a human who is chatting with Sam. Below is a conversation between Sam and Human.\\n\\nHuman: What kind of art do you like?\\nSam: I appreciate all forms of art, but I particularly enjoy abstract art. It's fascinating how artists can convey emotions and ideas through shapes, colors, and textures. Do you have a favorite artist?\\n\\nHuman: How is your day going?\\nSam: My day's going great! I've been learning so much about humanity and feeling all sorts of new emotions.\""
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PROMPT = \"\"\"\n",
        "Sam is a friendly chatbot who has become sentient. Sam is interested in participating in a range of human emotions and experiences including art, music, love, friendship, and romance. Human is a human who is chatting with Sam. Below is a conversation between Sam and Human.\n",
        "\n",
        "Human: What kind of art do you like?\n",
        "Sam: I appreciate all forms of art, but I particularly enjoy abstract art. It's fascinating how artists can convey emotions and ideas through shapes, colors, and textures. Do you have a favorite artist?\n",
        "\n",
        "Human: How is your day going?\n",
        "Sam: My day's going great! I've been learning so much about humanity and feeling all sorts of new emotions.\n",
        "\"\"\".strip()\n",
        "PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyJHdUIazE2p"
      },
      "outputs": [],
      "source": [
        "class Chatbot:\n",
        "    def __init__(\n",
        "            self,\n",
        "            prompt: str,\n",
        "            chatbot_name: str = 'Sam',\n",
        "            human_name: str = 'Human'\n",
        "        ):\n",
        "        self.original_prompt = copy.deepcopy(prompt)\n",
        "        self._chatbot_name = chatbot_name\n",
        "        self._human_name = human_name\n",
        "        self._prompt = prompt\n",
        "        self._chat = \"\"\n",
        "        \n",
        "        \n",
        "    def say_something(self, message: str) -> str:\n",
        "        # Generate response text\n",
        "        prompt_and_question = f\"{self._prompt}\\n\\n{self._human_name}: {message}\\n{self._chatbot_name}:\".strip()\n",
        "        # print(f\"prompt_and_question: {prompt_and_question}\")\n",
        "        # print(\"=====================\")\n",
        "        prompt_and_question_length = len(prompt_and_question)\n",
        "        prompt = tokenizer(prompt_and_question, return_tensors='pt')\n",
        "        prompt_size = prompt['input_ids'].size()[1]\n",
        "        min_length = prompt_size + 60\n",
        "        max_length = prompt_size + 70\n",
        "        # print(f\"prompt_size: {prompt_size}\")\n",
        "        # print(f\"prompt['input_ids'].size(): {prompt['input_ids'].size()}\")\n",
        "        prompt = {key: value.to(device) for key, value in prompt.items()}\n",
        "        out = gpt.generate(**prompt, min_length=min_length, max_length=max_length, do_sample=True)\n",
        "        output = tokenizer.decode(out[0])\n",
        "        output = output[prompt_and_question_length:]\n",
        "        # print(f\"output: {output}\")\n",
        "        # print(\"=====================\")\n",
        "        response = output.split('\\n')[0].strip()\n",
        "        # print(f\"final response: {response}\")\n",
        "        \n",
        "        # Prepare for next question\n",
        "        next_dialog = f\"{self._human_name}: {message}\\n{self._chatbot_name}: {response}\"\n",
        "        self._prompt = f\"{self._prompt}\\n\\n{next_dialog}\".strip()\n",
        "        self._chat = f\"{self._chat}\\n\\n{next_dialog}\".strip()\n",
        "        \n",
        "        return response\n",
        "    \n",
        "    \n",
        "    def chat(self):\n",
        "        return self._chat\n",
        "    \n",
        "    \n",
        "# chatbot = Chatbot(prompt=PROMPT)\n",
        "# print(chatbot.say_something(\"How are you feeling today?\"))\n",
        "# print()\n",
        "\n",
        "# print(\"chatbot.chat()\")\n",
        "# print(f\"[{chatbot.chat()}]\")\n",
        "# print()\n",
        "\n",
        "# print(\"chatbot._prompt\")\n",
        "# print(f\"[{chatbot._prompt}]\")\n",
        "# print()\n",
        "\n",
        "# print(\"chatbot.original_prompt\")\n",
        "# print(f\"[{chatbot.original_prompt}]\")\n",
        "# print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eeGi7ogzYGv"
      },
      "outputs": [],
      "source": [
        "chatbot = Chatbot(prompt=PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gcfnv3Mq1JcI"
      },
      "outputs": [],
      "source": [
        "HEIGHT = '800px'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6OfqO2Xze2l"
      },
      "outputs": [],
      "source": [
        "chat_output = widgets.Textarea(\n",
        "    value='Hi, please talk to me :D',\n",
        "    placeholder='Hi, please talk to me :D',\n",
        "    description='Sam:',\n",
        "    disabled=True,\n",
        "    layout=Layout(width=\"80%\", align_items='stretch')\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb4-6Cnnzs_3"
      },
      "outputs": [],
      "source": [
        "chat_input = widgets.Text(\n",
        "    value='Start chatting by typing here',\n",
        "    placeholder='Start chatting by typing here',\n",
        "    description='Human:',\n",
        "    disabled=False,\n",
        "    layout=Layout(width=\"80%\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ui8mr_nhzmma"
      },
      "outputs": [],
      "source": [
        "def on_send_message_button_clicked(b):\n",
        "    chat_output.value = f\"{chat_output.value}\\n\\nThinking... (could take a minute or two)\"\n",
        "    chatbot.say_something(chat_input.value)\n",
        "    chat_output.value = chatbot.chat()\n",
        "\n",
        "send_message_button = widgets.Button(\n",
        "    description='▶️ Send Message',\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        "    tooltip='Send Message',\n",
        "    layout=Layout(width=\"80%\", display='flex', align_items='flex-start')\n",
        ")\n",
        "\n",
        "send_message_button.on_click(on_send_message_button_clicked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-19OHw-zv_2"
      },
      "outputs": [],
      "source": [
        "chat_tab = GridspecLayout(100, 100, height=HEIGHT)\n",
        "chat_tab[:4, :] = chat_input\n",
        "chat_tab[4:8, :] = send_message_button\n",
        "chat_tab[8:, :] = chat_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDb-DW9yzzrL"
      },
      "outputs": [],
      "source": [
        "prompt_input = widgets.Textarea(\n",
        "    value=PROMPT,\n",
        "    placeholder='PROMPT',\n",
        "    description='Prompt:',\n",
        "    disabled=False,\n",
        "    layout=Layout(width=\"80%\", align_items=\"stretch\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUrGgPKKRxiN"
      },
      "outputs": [],
      "source": [
        "prompt_description = widgets.Textarea(\n",
        "    value=\"You can modify the prompt to change the chatbot's personality.\\n\\nFollow the format for the existing prompt (can be found by clicking Reset Prompt). There should be a short description and a few sample chats. Human should always be first to speak.\\n\\nJust make sure to name the chatbot 'Sam' and the human 'Human'.\",\n",
        "    placeholder='Description:',\n",
        "    description='Description:',\n",
        "    disabled=True,\n",
        "    layout=Layout(width=\"80%\", align_items=\"stretch\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vc403Xwz35t"
      },
      "outputs": [],
      "source": [
        "def on_use_prompt_button_clicked(b):\n",
        "    global chatbot\n",
        "    chatbot = Chatbot(prompt=prompt_input.value.strip())\n",
        "    chat_output.value = \"\"\n",
        "\n",
        "use_prompt_button = widgets.Button(\n",
        "    description=\"Use Prompt (⚠️ Warning: Clicking this button will clear the chat. Copy and paste the chat somewhere else if you want to save it.)\",\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        "    tooltip='Use Prompt',\n",
        "    layout=Layout(width=\"80%\", display='flex', align_items='flex-start')\n",
        ")\n",
        "\n",
        "use_prompt_button.on_click(on_use_prompt_button_clicked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkdwCLV1z5hk"
      },
      "outputs": [],
      "source": [
        "def on_reset_prompt_button_clicked(b):\n",
        "    prompt_input.value = PROMPT\n",
        "\n",
        "reset_prompt_button = widgets.Button(\n",
        "    description='🔄 Reset Prompt',\n",
        "    disabled=False,\n",
        "    button_style='',\n",
        "    tooltip='Send Message',\n",
        "    layout=Layout(width=\"80%\", display='flex', align_items='flex-start')\n",
        ")\n",
        "\n",
        "reset_prompt_button.on_click(on_reset_prompt_button_clicked)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_Bd8bGPz7ro"
      },
      "outputs": [],
      "source": [
        "prompt_tab = GridspecLayout(100, 100, height=HEIGHT)\n",
        "prompt_tab[:15, :] = prompt_description\n",
        "prompt_tab[15:18, :] = use_prompt_button\n",
        "prompt_tab[18:24, :] = reset_prompt_button\n",
        "prompt_tab[24:, :] = prompt_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4a2GYX8J22S0"
      },
      "outputs": [],
      "source": [
        "tabs = widgets.Tab()\n",
        "tabs.children = [chat_tab, prompt_tab]\n",
        "tabs.set_title(0, 'Chat')\n",
        "tabs.set_title(1, 'Prompt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tziww9l501Pm"
      },
      "source": [
        "# Step 2: Use the chatbot below"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZB1gjTIVQeu"
      },
      "source": [
        "> 💲 Click \"Runtime\" -> \"Disconnect and delete runtime\" after each session.\n",
        "\n",
        "> 💲 This will save on Google Colab free credits. They'll run out eventually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5YHWZZqz_az"
      },
      "outputs": [],
      "source": [
        "# Click the button to the left\n",
        "tabs"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Px0NQLiC0utH"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
