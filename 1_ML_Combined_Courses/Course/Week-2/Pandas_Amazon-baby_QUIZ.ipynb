{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Amazon baby data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data and remove NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "products = pd.read_csv(f\"D:/Docs/amazon_baby.csv\")\n",
    "#products = pd.read_csv(f\"D:/SYED/data/baby/amazon_baby.csv\")\n",
    "products = products.fillna({'review':''})  # fill in N/A's in the review column\n",
    "products['review'].isnull().values.any()\n",
    "products.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "products['review_clean'] = products['review'].apply(lambda x: x.translate(str.maketrans('', '', \n",
    "                           string.punctuation)))\n",
    "products.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rating of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = products[products['rating'] != 3]\n",
    "products.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More than 3 is +1 else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "products['review'][27401]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data as test and train\n",
    "* `train-idx.json` for Training data\n",
    "* `test-idx.json` for Test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data_indices = pd.read_json(f\"D:/repos/CourseraPlus/1_ML_Combined_Courses/Course/Week-2/train-idx.json\")\n",
    "train_data = pd.DataFrame(products, index = train_data_indices[0])\n",
    "test_data_indices = pd.read_json(f\"D:/repos/CourseraPlus/1_ML_Combined_Courses/Course/Week-2/test-idx.json\")\n",
    "test_data = pd.DataFrame(products, index = test_data_indices[0])\n",
    "print(\"Train set: \" + str(train_data.shape))\n",
    "print(\"Test set: \" + str(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate the test index list\n",
    "test_index = pd.read_json('test-idx.json', orient='values')\n",
    "test_index_list = [x[0] for x in test_index.values]\n",
    "train_data = products.copy()\n",
    "train_data = train_data.drop(train_data.index[test_index_list])\n",
    "\n",
    "# Train index\n",
    "train_index = pd.read_json('train-idx.json', orient='values')\n",
    "train_index_list = [x[0] for x in train_index.values]\n",
    "test_data = products.copy()\n",
    "test_data = test_data.drop(test_data.index[train_index_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove all the NaN before carrying out training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(subset = [\"name\"], inplace=True)\n",
    "train_data.isnull().values.sum()\n",
    "test_data.dropna(subset = [\"name\"], inplace=True)\n",
    "test_data.isnull().values.sum()\n",
    "print(\"Train set: \" + str(train_data.shape))\n",
    "print(\"Test set: \" + str(test_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary of word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(token_pattern = r'\\b\\w+\\b')\n",
    "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
    "test_matrix = vectorizer.transform(test_data['review_clean'])\n",
    "print(\"Train matrix: \" + str(train_matrix.shape))\n",
    "print(\"Test matrix: \" + str(test_matrix.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-efficients with value more than 0: 89423 out of 121539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SYED\\AppData\\Roaming\\Python\\Python38\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sentiment_model = LogisticRegression(random_state=0).fit(train_matrix, train_data['sentiment'])\n",
    "zero_elem = (sentiment_model.coef_ >= 0).sum()\n",
    "less_elem = (sentiment_model.coef_ < 0).sum()\n",
    "total_val = zero_elem + less_elem\n",
    "print(\"Co-efficients with value more than 0: \" + str(zero_elem) + \" out of \" + str(total_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Our Baby Girl Memory Book</td>\n",
       "      <td>Absolutely love it and all of the Scripture in...</td>\n",
       "      <td>5</td>\n",
       "      <td>Absolutely love it and all of the Scripture in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Wall Decor Removable Decal Sticker - Colorful ...</td>\n",
       "      <td>Would not purchase again or recommend. The dec...</td>\n",
       "      <td>2</td>\n",
       "      <td>Would not purchase again or recommend The deca...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>New Style Trailing Cherry Blossom Tree Decal R...</td>\n",
       "      <td>Was so excited to get this product for my baby...</td>\n",
       "      <td>1</td>\n",
       "      <td>Was so excited to get this product for my baby...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 name  \\\n",
       "59                          Our Baby Girl Memory Book   \n",
       "71  Wall Decor Removable Decal Sticker - Colorful ...   \n",
       "91  New Style Trailing Cherry Blossom Tree Decal R...   \n",
       "\n",
       "                                               review  rating  \\\n",
       "59  Absolutely love it and all of the Scripture in...       5   \n",
       "71  Would not purchase again or recommend. The dec...       2   \n",
       "91  Was so excited to get this product for my baby...       1   \n",
       "\n",
       "                                         review_clean  sentiment  \n",
       "59  Absolutely love it and all of the Scripture in...          1  \n",
       "71  Would not purchase again or recommend The deca...         -1  \n",
       "91  Was so excited to get this product for my baby...         -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_test_data = test_data[10:13]\n",
    "sample_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.16442713  -3.09477187 -10.31064777]\n"
     ]
    }
   ],
   "source": [
    "sample_test_matrix = vectorizer.transform(sample_test_data['review_clean'])\n",
    "scores_sample = sentiment_model.decision_function(sample_test_matrix)\n",
    "print(scores_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score and Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, -1, -1]\n",
      "[9.94316154e-01 4.33234261e-02 3.32757702e-05]\n"
     ]
    }
   ],
   "source": [
    "def check_score(input_array):\n",
    "    result = []\n",
    "    for x in input_array:\n",
    "        if x >= 0:\n",
    "            result.append(1)\n",
    "        else:\n",
    "            result.append(-1)\n",
    "    return result\n",
    "print(check_score(scores_sample))\n",
    "prob_val = sentiment_model.predict_proba(sample_test_matrix)\n",
    "print(prob_val[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full data sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_clean</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18081</th>\n",
       "      <td>50.921076</td>\n",
       "      <td>Infantino Wrap and Tie Baby Carrier, Black Blu...</td>\n",
       "      <td>I bought this carrier when my daughter was abo...</td>\n",
       "      <td>5</td>\n",
       "      <td>I bought this carrier when my daughter was abo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15702</th>\n",
       "      <td>45.972176</td>\n",
       "      <td>Baby Einstein Around The World Discovery Center</td>\n",
       "      <td>I am so HAPPY I brought this item for my 7 mon...</td>\n",
       "      <td>5</td>\n",
       "      <td>I am so HAPPY I brought this item for my 7 mon...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30582</th>\n",
       "      <td>44.630419</td>\n",
       "      <td>Graco FastAction Fold Jogger Click Connect Str...</td>\n",
       "      <td>Graco's FastAction Jogging Stroller definitely...</td>\n",
       "      <td>5</td>\n",
       "      <td>Gracos FastAction Jogging Stroller definitely ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Score                                               name  \\\n",
       "18081  50.921076  Infantino Wrap and Tie Baby Carrier, Black Blu...   \n",
       "15702  45.972176    Baby Einstein Around The World Discovery Center   \n",
       "30582  44.630419  Graco FastAction Fold Jogger Click Connect Str...   \n",
       "\n",
       "                                                  review  rating  \\\n",
       "18081  I bought this carrier when my daughter was abo...       5   \n",
       "15702  I am so HAPPY I brought this item for my 7 mon...       5   \n",
       "30582  Graco's FastAction Jogging Stroller definitely...       5   \n",
       "\n",
       "                                            review_clean  sentiment  \n",
       "18081  I bought this carrier when my daughter was abo...          1  \n",
       "15702  I am so HAPPY I brought this item for my 7 mon...          1  \n",
       "30582  Gracos FastAction Jogging Stroller definitely ...          1  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions = sentiment_model.predict(test_matrix)\n",
    "final_scores_data = vectorizer.transform(test_data['review_clean'])\n",
    "final_score = sentiment_model.decision_function(final_scores_data)\n",
    "df = pd.DataFrame(final_score, columns = ['Score'])\n",
    "df_test = test_data\n",
    "df_test.reset_index(drop = True, inplace = True)\n",
    "df2 = pd.concat([df, df_test], axis = 1)\n",
    "df_export = df2.sort_values(by='Score', ascending=False)\n",
    "df_export.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data (Top & Bot 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = df_export.head(20)\n",
    "df_top.to_csv('top20.csv')\n",
    "df_bottom = df_export.tail(20)\n",
    "df_bottom.to_csv('bottom20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix special: (133174, 20)\n",
      "Test matrix special: (33282, 20)\n"
     ]
    }
   ],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']\n",
    "vectorizer_word_subset = CountVectorizer(vocabulary = significant_words) # limit to 20 words\n",
    "train_matrix_word_subset = vectorizer_word_subset.fit_transform(train_data['review_clean'])\n",
    "test_matrix_word_subset = vectorizer_word_subset.transform(test_data['review_clean'])\n",
    "print(\"Train matrix special: \" + str(train_matrix_word_subset.shape))\n",
    "print(\"Test matrix special: \" + str(test_matrix_word_subset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Log-Reg model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "simple_model = LogisticRegression(random_state=0).fit(train_matrix_word_subset, train_data['sentiment'])\n",
    "zero_elem_simple = (simple_model.coef_ > 0).sum()\n",
    "print(zero_elem_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co-efficient dictionary for significant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = simple_model.coef_\n",
    "arr2 = sentiment_model.coef_\n",
    "arr1 = arr1.reshape(-1, 1)\n",
    "arr2 = arr2.reshape(-1, 1)\n",
    "list1 = arr1.tolist()\n",
    "list2 = arr2.tolist()\n",
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "# Check the coefficients for 20 words used in model\n",
    "output_dict = dict(zip(significant_words, list1))\n",
    "setiment_dict = dict(zip(vectorizer.get_feature_names_out(), list2))\n",
    "#print(setiment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare two dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Small</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love</td>\n",
       "      <td>[1.3646982275578259]</td>\n",
       "      <td>[1.5673927755779458]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great</td>\n",
       "      <td>[0.9448579595373243]</td>\n",
       "      <td>[1.2492114551129934]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>easy</td>\n",
       "      <td>[1.1885937648567766]</td>\n",
       "      <td>[1.3691891460112817]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>old</td>\n",
       "      <td>[0.08474509132707345]</td>\n",
       "      <td>[0.023880962694272163]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>little</td>\n",
       "      <td>[0.5188840456189335]</td>\n",
       "      <td>[0.5181713454106159]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>perfect</td>\n",
       "      <td>[1.5090928051705035]</td>\n",
       "      <td>[1.9868198542683997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>loves</td>\n",
       "      <td>[1.6736521477287714]</td>\n",
       "      <td>[1.6974621858567691]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>well</td>\n",
       "      <td>[0.5038990477325531]</td>\n",
       "      <td>[0.4816978993253936]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>able</td>\n",
       "      <td>[0.1906930998941367]</td>\n",
       "      <td>[0.3236821086715096]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>car</td>\n",
       "      <td>[0.05888698138807688]</td>\n",
       "      <td>[0.10270968120582619]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>broke</td>\n",
       "      <td>[-1.6563931583600129]</td>\n",
       "      <td>[-1.2279500947431579]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>less</td>\n",
       "      <td>[-0.20987187808873708]</td>\n",
       "      <td>[-0.3005242740518305]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>even</td>\n",
       "      <td>[-0.5119928973910935]</td>\n",
       "      <td>[-0.38268311383678283]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>waste</td>\n",
       "      <td>[-2.0305612413776535]</td>\n",
       "      <td>[-1.7446243560605152]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>[-2.347703114475959]</td>\n",
       "      <td>[-2.26534922419784]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>work</td>\n",
       "      <td>[-0.6247332519494697]</td>\n",
       "      <td>[-0.4014122988637002]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>product</td>\n",
       "      <td>[-0.3211322158208286]</td>\n",
       "      <td>[-0.15736631992471764]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>money</td>\n",
       "      <td>[-0.8987556647661311]</td>\n",
       "      <td>[-0.7764775927233659]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>would</td>\n",
       "      <td>[-0.36213731355047113]</td>\n",
       "      <td>[-0.300304478068321]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>return</td>\n",
       "      <td>[-2.1049232388251045]</td>\n",
       "      <td>[-1.4686190424304506]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word                   Small               Sentiment\n",
       "0           love    [1.3646982275578259]    [1.5673927755779458]\n",
       "1          great    [0.9448579595373243]    [1.2492114551129934]\n",
       "2           easy    [1.1885937648567766]    [1.3691891460112817]\n",
       "3            old   [0.08474509132707345]  [0.023880962694272163]\n",
       "4         little    [0.5188840456189335]    [0.5181713454106159]\n",
       "5        perfect    [1.5090928051705035]    [1.9868198542683997]\n",
       "6          loves    [1.6736521477287714]    [1.6974621858567691]\n",
       "7           well    [0.5038990477325531]    [0.4816978993253936]\n",
       "8           able    [0.1906930998941367]    [0.3236821086715096]\n",
       "9            car   [0.05888698138807688]   [0.10270968120582619]\n",
       "10         broke   [-1.6563931583600129]   [-1.2279500947431579]\n",
       "11          less  [-0.20987187808873708]   [-0.3005242740518305]\n",
       "12          even   [-0.5119928973910935]  [-0.38268311383678283]\n",
       "13         waste   [-2.0305612413776535]   [-1.7446243560605152]\n",
       "14  disappointed    [-2.347703114475959]     [-2.26534922419784]\n",
       "15          work   [-0.6247332519494697]   [-0.4014122988637002]\n",
       "16       product   [-0.3211322158208286]  [-0.15736631992471764]\n",
       "17         money   [-0.8987556647661311]   [-0.7764775927233659]\n",
       "18         would  [-0.36213731355047113]    [-0.300304478068321]\n",
       "19        return   [-2.1049232388251045]   [-1.4686190424304506]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compare_ndic(src, dest):\n",
    "    result = []\n",
    "    for skey, sval in src.items():\n",
    "        for nkey, nval in dest.items():\n",
    "            if skey == nkey:\n",
    "                result.append([skey, sval, nval])\n",
    "    return result\n",
    "\n",
    "new_list = compare_ndic(output_dict, setiment_dict)\n",
    "df_dict = pd.DataFrame(new_list, columns=['Word', 'Small', 'Sentiment'])\n",
    "df_dict.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare `train` & `test` accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment model accuracy TRAINING data:\n",
      "0.9324559822126074\n",
      "Simple model accuracy TRAINING data:\n",
      "0.8692987200288445\n"
     ]
    }
   ],
   "source": [
    "accuracy_sentiment_model = sentiment_model.score(final_scores_data, test_data['sentiment'].values)\n",
    "print(\"Sentiment model accuracy TRAINING data:\\n\" + str(accuracy_sentiment_model))\n",
    "\n",
    "accuracy_simple_model = simple_model.score(test_matrix_word_subset, test_data['sentiment'].values)\n",
    "print(\"Simple model accuracy TRAINING data:\\n\" + str(accuracy_simple_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    28048\n",
       "-1     5234\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority classifier accuracy should be:\n",
      "0.8427378162370049\n"
     ]
    }
   ],
   "source": [
    "print(\"Majority classifier accuracy should be:\\n\" + str(28048 /(28048 + 5234)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
