{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DGPlYumZnO1t"
   },
   "source": [
    "# Exploratory Data Analysis Using Python and BigQuery\n",
    "\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Analyze a Pandas Dataframe\n",
    "2. Create Seaborn plots for Exploratory Data Analysis in Python \n",
    "3. Write a SQL query to pick up specific fields from a BigQuery dataset\n",
    "4. Exploratory Analysis in BigQuery\n",
    "\n",
    "\n",
    "## Introduction \n",
    "This lab is an introduction to linear regression using Python and Scikit-Learn.  This lab serves as a foundation for more complex algorithms and machine learning models that you will encounter in the course. We will train a linear regression model to predict housing price.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in this student lab notebook -- try to complete this notebook first and then review the [solution notebook](../solutions/python.BQ_explore_data.ipynb). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsHg6SD2nO1v"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user google-cloud-bigquery==1.25.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please ignore any incompatibility warnings and errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restart** the kernel before proceeding further (On the Notebook menu - Kernel - Restart Kernel).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gEXV-RxPnO1w"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# delete me from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns # Seaborn is a Python data visualization library based on matplotlib. \n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dr2TkzKRnO1z"
   },
   "source": [
    "###  Load the Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a directory called usahousing.  This directory will hold the dataset that we copy from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(\"../data/explore\"):\n",
    "    os.makedirs(\"../data/explore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we copy the Usahousing dataset from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil cp gs://cloud-training/mlongcp/v3.0_MLonGC/toy_data/housing_pre-proc_toy.csv ../data/explore  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the \"ls\" command to list files in the directory.  This ensures that the dataset was copied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l ../data/explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read the dataset into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzrXJI8VnO10"
   },
   "outputs": [],
   "source": [
    "df_USAhousing = pd.read_csv('../data/explore/housing_pre-proc_toy.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "Y6VJQ1tdnO12",
    "outputId": "7a1d4eed-3e83-44a8-f495-a9b74444d3ec"
   },
   "outputs": [],
   "source": [
    "# Show the first five row.\n",
    "\n",
    "df_USAhousing.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check for any null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_USAhousing.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "nRTsvSzqnO17",
    "outputId": "f44ad14e-5fb4-4c70-e71c-9d149bca4869"
   },
   "outputs": [],
   "source": [
    "df_stats = df_USAhousing.describe()\n",
    "df_stats = df_stats.transpose()\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_USAhousing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a peek at the first and last five rows of the data for all columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Rows     : \" ,df_USAhousing.shape[0])\n",
    "print (\"Columns  : \" ,df_USAhousing.shape[1])\n",
    "print (\"\\nFeatures : \\n\" ,df_USAhousing.columns.tolist())\n",
    "print (\"\\nMissing values :  \", df_USAhousing.isnull().sum().values.sum())\n",
    "print (\"\\nUnique values :  \\n\",df_USAhousing.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QWVdsrmgnO1_"
   },
   "source": [
    "## Explore the Data\n",
    "\n",
    "Let's create some simple plots to check out the data!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df_USAhousing.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a displot showing \"median_house_value\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "colab_type": "code",
    "id": "SOsTLClWnO2B",
    "outputId": "b8a78674-5ddb-4706-90b4-37d7d83e8092"
   },
   "outputs": [],
   "source": [
    "sns.displot(data=df_USAhousing, x=\"median_house_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "df_USAhousing['median_house_value'].hist(bins=30)\n",
    "plt.xlabel('median_house_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_USAhousing['median_income']\n",
    "y = df_USAhousing['median_house_value']\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a jointplot showing \"median_income\"  versus \"median_house_value\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=df_USAhousing, x=\"median_income\", y=\"median_house_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x = 'ocean_proximity', data=df_USAhousing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes numeric only?\n",
    "#plt.figure(figsize=(20,20))\n",
    "g = sns.FacetGrid(df_USAhousing, col=\"ocean_proximity\")\n",
    "g.map(plt.hist, \"households\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes numeric only?\n",
    "#plt.figure(figsize=(20,20))\n",
    "g = sns.FacetGrid(df_USAhousing, col=\"ocean_proximity\")\n",
    "g.map(plt.hist, \"median_income\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see below that this is the state of California!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_USAhousing['latitude']\n",
    "y = df_USAhousing['longitude']\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore and create ML datasets\n",
    "\n",
    "In this notebook, we will explore data corresponding to taxi rides in New York City to build a Machine Learning model in support of a fare-estimation tool. The idea is to suggest a likely fare to taxi riders so that they are not surprised, and so that they can protest if the charge is much higher than expected.\n",
    "\n",
    "## Learning objectives\n",
    "* Access and explore a public BigQuery dataset on NYC Taxi Cab rides\n",
    "* Visualize your dataset using the Seaborn library\n",
    "\n",
    "\n",
    "First, **restart the Kernel**.  Now, let's start with the Python imports that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Extract sample data from BigQuery </h3>\n",
    "\n",
    "The dataset that we will use is <a href=\"https://console.cloud.google.com/bigquery?project=nyc-tlc&p=nyc-tlc&d=yellow&t=trips&page=table\">a BigQuery public dataset</a>. Click on the link, and look at the column names. Switch to the Details tab to verify that the number of records is one billion, and then switch to the Preview tab to look at a few rows.\n",
    "\n",
    "Let's write a SQL query to pick up interesting fields from the dataset. It's a good idea to get the timestamp in a predictable format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "SELECT\n",
    "    FORMAT_TIMESTAMP(\n",
    "        \"%Y-%m-%d %H:%M:%S %Z\", pickup_datetime) AS pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, dropoff_longitude,\n",
    "    dropoff_latitude, passenger_count, trip_distance, tolls_amount, \n",
    "    fare_amount, total_amount \n",
    "# TODO 3: Set correct BigQuery public dataset for nyc-tlc yellow taxi cab trips\n",
    "# Tip: For projects with hyphens '-' be sure to escape with backticks ``\n",
    "FROM `nyc-tlc.yellow.trips`\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's increase the number of records so that we can do some neat graphs.  There is no guarantee about the order in which records are returned, and so no guarantee about which records get returned if we simply increase the LIMIT. To properly sample the dataset, let's use the HASH of the pickup time and return 1 in 100,000 records -- because there are 1 billion records in the data, we should get back approximately 10,000 records if we do this.\n",
    "\n",
    "We will also store the BigQuery result in a Pandas dataframe named \"trips\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery trips\n",
    "SELECT\n",
    "    FORMAT_TIMESTAMP(\n",
    "        \"%Y-%m-%d %H:%M:%S %Z\", pickup_datetime) AS pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, \n",
    "    dropoff_longitude, dropoff_latitude,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    tolls_amount,\n",
    "    fare_amount,\n",
    "    total_amount\n",
    "FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 100000)) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can slice Pandas dataframes as if they were arrays\n",
    "trips[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Exploring data </h3>\n",
    "\n",
    "Let's explore this dataset and clean it up as necessary. We'll use the Python Seaborn package to visualize graphs and Pandas to do the slicing and filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: Visualize your dataset using the Seaborn library.\n",
    "# Plot the distance of the trip as X and the fare amount as Y.\n",
    "ax = sns.regplot(x=\"trip_distance\", y=\"fare_amount\", fit_reg=False, ci=None, truncate=True, data=trips)\n",
    "ax.figure.set_size_inches(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm ... do you see something wrong with the data that needs addressing?\n",
    "\n",
    "It appears that we have a lot of invalid data that is being coded as zero distance and some fare amounts that are definitely illegitimate. Let's remove them from our analysis. We can do this by modifying the BigQuery query to keep only trips longer than zero miles and fare amounts that are at least the minimum cab fare ($2.50).\n",
    "\n",
    "Note the extra WHERE clauses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery trips\n",
    "SELECT\n",
    "    FORMAT_TIMESTAMP(\n",
    "        \"%Y-%m-%d %H:%M:%S %Z\", pickup_datetime) AS pickup_datetime,\n",
    "    pickup_longitude, pickup_latitude, \n",
    "    dropoff_longitude, dropoff_latitude,\n",
    "    passenger_count,\n",
    "    trip_distance,\n",
    "    tolls_amount,\n",
    "    fare_amount,\n",
    "    total_amount\n",
    "FROM\n",
    "    `nyc-tlc.yellow.trips`\n",
    "WHERE\n",
    "    ABS(MOD(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)), 100000)) = 1\n",
    "    # TODO 4a: Filter the data to only include non-zero distance trips and fares above $2.50\n",
    "    AND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(trips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(\n",
    "    x=\"trip_distance\", y=\"fare_amount\",\n",
    "    fit_reg=False, ci=None, truncate=True, data=trips)\n",
    "ax.figure.set_size_inches(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's up with the streaks around 45 dollars and 50 dollars?  Those are fixed-amount rides from JFK and La Guardia airports into anywhere in Manhattan, i.e. to be expected. Let's list the data to make sure the values look reasonable.\n",
    "\n",
    "Let's also examine whether the toll amount is captured in the total amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tollrides = trips[trips[\"tolls_amount\"] > 0]\n",
    "tollrides[tollrides[\"pickup_datetime\"] == \"2012-02-27 09:19:10 UTC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notollrides = trips[trips[\"tolls_amount\"] == 0]\n",
    "notollrides[notollrides[\"pickup_datetime\"] == \"2012-02-27 09:19:10 UTC\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a few samples above, it should be clear that the total amount reflects fare amount, toll and tip somewhat arbitrarily -- this is because when customers pay cash, the tip is not known.  So, we'll use the sum of fare_amount + tolls_amount as what needs to be predicted.  Tips are discretionary and do not have to be included in our fare estimation tool.\n",
    "\n",
    "Let's also look at the distribution of values within the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc.  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "http://www.apache.org/licenses/LICENSE-2.0\n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Conchita_Linear_Regression_with_Python.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
